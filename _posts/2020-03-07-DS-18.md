---
title: "Big Data, MapReduce and Hadoop"
tags: [Distributed Systems]
categories:
  - Distributed Systems
date: 2020-03-07
---


## **18. Big Data, MapReduce and Hadoop**

**Goal** – Understand concepts of MapReduce/Hadoop
  - And why data has become so important
  - Dealing with large data
  - Parallel data processing

**Big Data Everywhere**
  - Not a single data where data is not collected and stored in a
    warehouse
      - a lot of them is ecommerce, web data
      - grocery shop, and pay with credit card transaction data
  - Data needs to be <span class="underline">managed, analysed,
    summarised, virtualised</span>, so that we can discover knowledge
    from these data → imp for business, and trend on fb for advert
  - And you want to do it in a <span class="underline">timely
    manner</span> and scalability is key, you expect to deal with these
    data in massive scale

**How much data? – daily generation**
  - from different commerce and website.
      - Google – how much it processes everyday – 100PBs. And 15
        Exabytes
      - Facebook – 300PB and 600TB a day
      - CERN – European nuclear center – 40PBs year – where they
        experiment with ‘large hadron collider’
          - Bc the data needs to be generated, stored and analysed

**Types of data**

  - What type of data would you be dealing with on a daily basis?
  - Relational data - Tables, transaction, legacy
  - Text data - Web text
  - Semi-Structured data – XML, JSON
  - Graph data – social network from the semantic web.
  - Streaming data – video stream and you analyse when its coming
      - V imp for computer vision for surveillance and security

**What to do with these data?**

  - Is used for searching and querying algorithm
      - Keyword based search
      - Pattern matching – you try to find similar pattern in other
        data.
          - E.g. aircraft engine – pattern matching is imp coz its imp
            to understand for maintenance and detecting faults in the
            engine
  - Knowledge discovery
      - Data mining
      - Statistical modelling
  - Statistics and aggregation
      - Data is stored in a warehouse

**What is MapReduce?**

  - Data comes in a large scale and MapReduce is a programming model to
    express distributed computations at a massive scale. you want to
    process it parallelly hence you need this programming model.
  - Not only it does the processing, but it will organize and preform
    search computations so that the process takes minimum time.
  - Been introduced with google in 2003, and there's open source
    implementation called Hadoop

**Is it important?**

  - Process lots of data in parallel
  - Large ready-made data problem
      - Data whether it is user generated, cloud provider, we want to do
        sth with that data to make sense of it
  - We can see the appetite of providing <span class="underline">utility
    computing</span>
      - Taking this idea of large data processing and making it
        available as a service as utility computing → provision of
        parallel data processing clusters
      - If you give me data, I can provide you this utility computing
        service and can help you make sense of the data

**How do we scale up? Divide and Conquer**

  - It comes from battlefield\! And politicians
  - The way we scale is we have work to do and we partition with small
    chunks, and we assign partitions to workers and they work, and we
    get intermediate result r1 r2 r3 and combine it to get the final
    result
  - If we can apply this idea for large parallel processing →
    MapReduce is about\!

**Parallelization challenges**

  - Few questions arise to make this happen.
  - How do we assign work units to workers?
  - What if we have more work than workers?
  - What if workers need to share partial results? → May need to
    synchronize
  - How do we aggregate partial results?
  - How do we know if they’ve finished? → some form of synchronization
  - What if workers die?
      - Fault tolerance, need to detect AND need a mechanism to get the
        work restarted
  - MapReduce has to deal with these problems

**Common theme?**

  - you have number of problems that arise bc they need to communicate
    with each other and share resource → therefore you need
    synchronization mechanism\!\!

**Managing multiple workers**

  - Difficult coz we don’t know
      - The order in which workers run
      - When the workers interrupt each other
      - And the order in which workers access shared data
  - Thus we need → OS\!\!
      - Semaphores – lock and unlock (deadlock)
      - Conditional variables (wait, notify, broadcast)
      - Barriers
  - Still have problems
      - Deadlocks, race conditions for memory and CPU
      - Dining philosophers – 5 philosophers sit around the table and
        they eat and think
          - Resource sharing problems

**Current tools**

- In the world of parallel processing you have two models:
    1.  Shared memory - have 5 processes and they access same memory and the access needs to be controlled

    2.  MPI - every process has its own private memory and they do
        communication via message passing

- Design patterns

  1.  Master slaves – imp for MapReduce

  2.  Producer-consumer flaws – in OS, they have processors and do work, and put it in the queue/ buffer, and consumers will go to buffers and take what they produced
      - Producers cannot produce if buffer is full and consumers cannot access if the buffer is empty
  3.  Work queue – ActiveMQ, and modern systems use this, where processors
      this one is synchronised with what's in the queue – lecture on
      communication


**Challenges**

  - Concurrency is difficult to reason bc
      - Coz the scale of data center, and across the world – you need to
        deal with failures and you have thousands of services and users
        up and running. → problem on a large scale
  - Reality
      - One off solution – can write own code
      - Can run dedicated library
      - Burden is that you have to manage everything – design,
        implementation, concurrency

**Big ideas**

  - I'm gonna hide system level detail
      - Deadlock is gone, no race condition.
  - Separate what from how
  - As a developer you specify the computation (what) and MapReduce will
    handle the actual execution
  - Keep in mind to move the processing to data\!\!\!
      - We have massive data and some form of cluster, and if we move
        data to processing, we have to deal with bandwidth and latency
        issue. So instead we move the processing
  - Processing the data sequentially, and can deal with how often you
    access the disk bc the seeks are expensive yet disk throughput is
    quite reasonable

**Typical large-data problem**

1.  Take massive data and iterate over a large number of records
2.  Extract sth of interest → MAP
3.  Shuffle intermediate results
4.  Aggregate intermediate results → REDUCE
5.  Generate final output
<!-- end list -->
  - Key idea – map by extracting, and aggregate the result to reduce\!\!
  - Provide a functional abstraction for these two operations


**MapReduce**
![image](https://user-images.githubusercontent.com/33334078/75290761-de48e500-5818-11ea-8030-23de41dc19ea.png)

  - My code needs exclusively 2 functions in the code - Map and Reduce
  - **Map** – take a key and associate value to it. And we reduce all
    the values with same keys to the same worker
  - And you end up with some form of intermediate results that says
    mapping of key v1 and k1 has led to a1 b2 etc.
  - As you could see, it looks like a b and c appeared a number of times
    as intermediate results. Hence, we shuffle and sort out aggregated
    values by keys. A with 1,5 and b with 2,7 and c with 2,3,6,8.
  - So that they can get to final reduction operation
  - All this is done \!\! by MapReduce so we needa write Map and Reduce.

**MapReduce “Runtime”**

  - Runtime handles scheduling – assign workers to map and reduce tasks
  - Handles data distribution - moves process to data
  - Handle synchronization – gathers, sorts and shuffles intermediate
    data
  - Handle errors and faults – detects failure and restart if needed

**MapReduce**

  - Can add.2 more functions - Partition and Combine
  - If you know the data you are dealing with, you can look at number of
    partitions as an important parameter in ur code. you can then then
    can divide up the key space for parallel reduce operation, as you
    know the key to divide with
  - And can for performance reason, run mini reduces that run-in memory
    after the map phase. Once you do the map and start generating data,
    you can already start the reduction operation and combine
    intermediate result
      - For optimization reason - to reduce network traffic when sharing
        intermediate result
  - → 2 extra thing if you know the data

**Logical data flow in 5 processing steps in MapReduce**

  - Input data has a Key and the value as a pair.
  - Map function generates these pairs. And I'm gonna sort out key value
    pairs, sort by key. Then you group them so key 1 has n values and
    key x has n values.
  - We reduce and we get the final output
  - → Sort and group

**A world counting example on \<Key, Count\> distribution**

  - ![Screenshot 2020-02-25 at 9 51 31 pm](https://user-images.githubusercontent.com/33334078/75290828-fcaee080-5818-11ea-9d67-ecac60c22771.png)
  - Use MapReduce to count the number of words in a book
  - As an input, you have ‘most ppl ignore most poetry’ and ‘most
    poetry ignore most ppl’
  - Map function will count the individual words as keys. All these keys
    are individual words
  - you sort them out and group them
      - you realize ignore - 1 instance, ignores- 1 instance
  - By reducing, you can count the number of instances in each unique
    keys to find out how many times each words have been mentioned.

**MapReduce Implementations**

  - Have google, which had a propriety implementation in C++ and
    provided bindings in Java and Python
  - Hadoop – open source implementation written in Java by apache, then
    yahoo
      - Rapidly expanding, and there's tons of variance. i.e. spark.
        Spark claims to be 100 times faster than Hadoop as it runs on
        memory
  - Lots of custom research implementations
      - If you use GPU as accelerators, you can add custom
        implementation of Hadoop itself

**Distributed file system**

  - MapReduce will work, but the underlying system, you need distributed
    file system – you need to arrange data so that Hadoop can access and
    work with it and make that data available to the framework
  - The data itself can be stored in local disks of nodes in the cluster
      - The data is first stored on data nodes and is spread across the
        cluster in school test bed
      - you start up the workers on the node that has the local data, aka
        that is closer to data

  - Why?
      - Not enough RAM to hold all the data in memory and is better for
        latency
      - Disk access is slow but disk throughput is reasonable

  - Distributed file system is the answer\!
      - GFS (Google File System)– google’s version of MapReduce
      - HDFS (Hadoop Distributed File System) – to prepare data for
        parallel processing

**Case study – GFS**

  - Would rely on data being available on clusters, and these clusters
    will have racks and each rack has number of servers
  - Clusters need to communicate with each other through switches
  - Number of datacentres all over the world connected through network



**Data center networks**

  - ![image](https://user-images.githubusercontent.com/33334078/75290848-07697580-5819-11ea-8708-5a2d9d1d1ae1.png)
  - Where you do a quick search and it goes down to border router → load
    balancer → get to certain rack through switches
  - Load balancer receives request and directs workload within data
    center and returns result to client
  - TOR – top of the rack 보드 to do with the way the networking is set up
    and you have very hierarchical view of switches
      - Communication has to be extremely fast – theoretically 5
        microseconds
  - When you are processing the data, some rack could talk to the one
    next, and to ask for more computation and synchronize and work
    together for my request

**GFS**

  - Assumptions
      - Google always relied on commodity hardware
      - Expected failure to be common.
      - Have modest number of huge files and multi gigabyte files are
        common
      - Files are write once- but files may be appended


  - How do they design gfs?
      - ![Screenshot 2020-02-25 at 9 52 09 pm](https://user-images.githubusercontent.com/33334078/75290874-13edce00-5819-11ea-82bb-6b8408b77ee1.png)
      - Expect a GFS client looking for some data. It calls a GFS master
        which has metadata aka which is where the data is. It then calls
        a GFS <span class="underline">chunk</span> server. Interesting
        is bc of fault tolerance, the data is
        <span class="underline">replicated</span> at least 3 times. Once
        the data is found, the chunk is passed back to client
      - There's a single master to coordinate access
      - Caching doesn’t exist – google claims says there's no point in
        large dataset
      - But in this case there's a single point of failure – the master
        node. And google says that google can detect that master node
        has failed, and they have a system that you really quickly
        assign a new master

**Summary**

  - Cloud computing and large data
  - General idea of MapReduce and about large data processing
  - Importance of underlying distributed file systems
  - GFS as a case study

<!-- end list -->
