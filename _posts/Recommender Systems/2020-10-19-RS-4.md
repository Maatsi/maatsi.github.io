---
title: "User Models and Profiles (building)"
tags: [Recommender Systems]
categories:
  - Recommender Systems
date: 2020-10-19
---


1. [User Information Collection](#user-information-collection)    
  1.1 [Explicit user information collection](#explicit-user-information-collection)   
  1.2 [Implicit user information collection](#implicit-user-information-collection)    
  1.3 [Techniques for implicit user information collection](#techniques-for-implicit-user-information-collection)  
2. [Step 1. User identification](#step-1-user-identification)  
3. [Step 2. User Model Construction](#step-2-user-model-construction)  
  3.1 [Building keyword-based user model](#building-keyword-based-user-model)  
  3.2 [Building graph-based user model](#building-graph-based-user-model)  
  3.3 [Building concept-based user model](#building-concept-based-user-model)  
4. [Summary](#summary)


## User Information Collection

### Explicit user information collection
  - Information entered by the user, via HTML forms (self-report or
    self-assessment)
  - Data contains:
      - Demographics such as bday, marriage, hob, personal status
  - Explicit feedback
      - Rates some links on a page (e.g. Syskill \&Webert) it recommends
        other links which they might be interested in

<table>
<thead>
<tr class="header">
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><ul>
<li><p>More reliable information</p></li>
<li><p>Easy to process</p></li>
<li><p>Complies with privacy regulations</p></li>
<li><p>User is in control of what info they give out</p></li>
</ul></td>
<td><ul>
<li><p>Users have to voluntarily provide information; otherwise no profile can be built</p></li>
<li><p>Requires time and willingness to contribute</p></li>
<li><p>Places additional burden on the user and can take long hence can lower user experience</p></li>
<li><p>User may be confused or biased in information they give</p></li>
<li><p>There may be information outside of what user knows</p></li>
<li><p>Dynamic changes can be missed – people change and you have to ask multiple times inaccuracy overtime</p></li>
</ul></td>
</tr>
</tbody>
</table>


### Implicit user information collection
  - Constructed based on implicitly collected information – implicit
    user feedback
  - Is collected by the system
      - Collected on user’s client machine or application server
  - Uses digital traces of UI – comments we write, news we read, items
    we click, video we watch
  - May add additional information about user device
  - user is not explicitly aware that we are collecting information

<table>
<thead>
<tr class="header">
<th><strong>Pros</strong></th>
<th><strong>Cons</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><ul>
<li><p>Not obtrusive - Does not require any additional intervention by the user</p></li>
<li><p>Doesn’t require new software to be developed and installed</p></li>
<li><p>Rich data about the user</p></li>
<li><p>Gather information quickly</p></li>
</ul></td>
<td><ul>
<li><p>Not all personalised sites are used frequently enough by any single user to allow them to create a useful profile</p></li>
</ul></td>
</tr>
</tbody>
</table>


### Techniques for implicit user information collection
<table>
<thead>
<tr class="header">
<th><strong>Technique</strong></th>
<th><strong>Information collected</strong></th>
<th><strong>Information breadth</strong></th>
<th><strong>Pros</strong></th>
<th><strong>Cons</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Browser cache</strong></td>
<td>Browsing history</td>
<td>Any Websites</td>
<td><ul>
<li><p>Unobtrusive – user doesn’t have to install anything</p></li>
<li><p>Captures history</p></li>
<li><p>Objective/ authentic</p></li>
</ul></td>
<td><ul>
<li><p>Privacy invasion</p></li>
<li><p>Noisy (more than one user using the device)</p></li>
</ul></td>
</tr>
<tr class="even">
<td><strong>Interaction/ desktop agents</strong></td>
<td><p>Interaction and user activity</p>
<ul>
<li><p>All the steps within the app</p></li>
</ul></td>
<td>Any personalised application</td>
<td><ul>
<li><p>All user files and activity available</p></li>
<li><p>Options they click on, resources they are opening</p></li>
</ul></td>
<td><ul>
<li><p>Requires user to install software</p></li>
<li><p>Investment in development of the software</p></li>
</ul></td>
</tr>
<tr class="odd">
<td><strong>Logs (web logs, search logs)</strong></td>
<td><p>Browsing/ search activity</p>
<ul>
<li><p>Pages user clicked</p></li>
<li><p>Keywords</p></li>
</ul></td>
<td>Websites/ search engine sites that are logged</td>
<td><ul>
<li><p>If you can link the word’s meanings to concept, it can be quite clever – knowledge graph of google (airport-travel)</p></li>
<li><p>Information about multiple users collected</p></li>
<li><p>Collection and use of information all at same time</p></li>
</ul></td>
<td><ul>
<li><p>May be very little information as it is from one site</p></li>
<li><p>Cookies must be turned on/ and or login to site</p></li>
</ul></td>
</tr>
<tr class="even">
<td><strong>File transfer from one app to another</strong></td>
<td><p>Previously stored information</p>
<ul>
<li><p>Wishlist on YouTube/ Netflix</p></li>
</ul></td>
<td>Application/ organisation specific</td>
<td><ul>
<li></li>
</ul></td>
<td>Has to be something that already exists so could not be much</td>
</tr>
<tr class="odd">
<td><strong>Mobile/ wearable sensors</strong></td>
<td>Contextual such as GPS; physiological &amp; psychological states</td>
<td>Anywhere / anytime the user has the devices on</td>
<td><ul>
<li><p>Various information and real time data</p></li>
</ul></td>
<td><ul>
<li><p>User has to have the device on</p></li>
</ul></td>
</tr>
<tr class="even">
<td>Emerging – <strong>speech/ comments on social media</strong></td>
<td>Sentiments, viewpoints, interests</td>
<td>Social media platforms</td>
<td><ul>
<li><p>Text, pitch, hmm kinda thing, pace, together with the language can be powerful</p></li>
<li><p>Language gives authoritative information / indication</p></li>
<li><p>Natural communication</p></li>
<li><p>Fairly robust technique – there are libraries that allow processing of speech reliably</p></li>
</ul></td>
<td><ul>
<li><p>Privacy and security</p></li>
</ul>
<p>Email dictation – emails now reside somewhere in the server</p>
<ul>
<li><p>Can disadvantage some people with handicap – need to offer another door</p></li>
<li><p>Some languages may not be well resourced</p></li>
<li><p>Environment restricted – noisy</p></li>
</ul></td>
</tr>
</tbody>
</table>


## Step 1. User identification
  - Once you decide the information collection method, who is the user?
    How do we identify the user we are building the model for?
  - Crucial for any system that constructs profiles that represent
    individual users

  - **Methods for user identification**
    - **Software agents**
        - Small programs that reside on the user’s computer
        - Collect information about the user and share with a server
          via some protocol
        - \+ 1<sup>st</sup> reliable - more control over the
          implementation and protocol used for identification
        - \- requires user participation to install the software

    - **Logins**
        - \+ Better accuracy and consistency – track across sessions
          and btwn computers
        - \+ Can access information from different computers
        - \+ Knows who the user is and can control who they are
        - \+ With user consent consented
        - 2<sup>nd</sup> reliable
        - \- user must create an account via registration, login and
          log out burden on
          user

    - **Cookies**

        - Easiest and most widely deployed – transparent to the user
        - \- Poor accuracy due to multiple users – then it becomes
          privacy violation
        - \- if user uses more than one computer, it will create
          separate user profile
        - \- if user clears cookie its reset

    - **Session IDs**

        - Activity during the visit is tracked
        - \+ All the browsers are using it
        - \+ Good for searches – look at the session for short time
          and start recommending (adapting)
        - \+ Doesn’t violate privacy – no need to record bc you are only
          looking at the current session
        - \- Not a long-term user model

    - **Enhanced proxy servers**
        - \- Require that users register their machines with proxy
          server
        - \- Generally only able to identify users connecting from
          only one location, unless they bother registering diff comps
          with same proxy





## Step 2. User model construction
- Next step of constructing a user model Need to think about techniques we are going to use

1.  We need to take input information about the user – data mining
    skills.
2.  Also need to take into account not only what comes in but
    <span style="text-decoration:underline">what you are going to look in the user
    model</span> (**user model representation**)– which part is related
    to my user model
      - E.g. modelling emotional state – what info I captured is related
        to this?
      - What's the model, what comes in, which of the info will give me
        the final info model?
3.  Conduct appropriate processing – taking the info and need to derive
    processing to come up with a model
      - If the model is binary, e.g. if the user is active or inactive –
        this could become a **classifier**
      - If you are looking for several parameters – might need to do
        other processing,
          - One way is to overlay the user model – aggregating user
            model – looking at frequencies or inferences
4.  Extract the user model\! Final outcome


### Building keyword-based user model

- ![Picture 1](https://user-images.githubusercontent.com/33334078/95707971-085ef280-0c96-11eb-82bf-db74a152def6.png)
- ![Picture 12](https://user-images.githubusercontent.com/33334078/95707979-0b59e300-0c96-11eb-910b-73856dcf7ea9.png)

1.  Initially created by extracting keywords from web pages collected
    from some information source (e.g. browsing history) - The user is
    browsing the web
2.  Through **browsing agent,** If the user is clicking the document,
    then you pull the document
3.  From these **positive feedback documents**, look at the text it has
    which is what user have possibly read
    - Positive feedback document – represents user’s interest
4.  **From these documents, extract keywords** of the document and
    **weight** them using **TF\*IDF** (Term frequency inverse document
    frequency)


**Input and output**
- Input - Unpacking the documents, which is what user has read
  - What we want – **list of keywords** k1,k2,k3,…

**Steps**
1.  **TF -** We unpack the documents then count the frequency for
    each word in each of the document
    - Some terms will only be in specialised documents more
        important\!
2.  **IDF -** Inverse document frequency – count how many of these
    documents represent a particular term, for all terms in all
    documents
    - Which tells the **weight of this term** in the document
        space\!
3.  **TFIDF** Multiply both to find TFIDF
    - Title and heading words are identified and weighed more
        highly
- Term with highest TFIDF core terms. And we need smart ways of
aggregating these core terms. i.e. based on similarities, based on
overlap of the language
- then there's a user model \!\!




### Building graph-based user model
  - ![image](https://user-images.githubusercontent.com/33334078/95708001-20cf0d00-0c96-11eb-9cbd-2191515a7e0d.png)
- Built by collecting explicit positive and negative feedback from
    users

- **Input**: graph

- What are the user interests from the documents that we pulled this
  is the POSTIVIE Example of user interests
    - We are reliant on having reliable enough method to identify that
      a document is a positive i.e. how long they stayed, if they’ve
      shared, etc

- Reminder: entities to be the nodes, and we have relationships
  between the nodes. We want to extract this graph

- **Graph overlay** – overlay the entities that users are interested
  in output
    - From the document, you need to be looking for **concepts** from
      the graph – world knowledge is usually given, so rather than
      counting the term, in here we look for concepts that are part of
      this graph need a diff approach \!\!

- Approach we use is **semantic tagging**
    - There are libraries, and tools
    - Take world knowledge from the world model and map it, and go
      through textural documents, and identify which of the annotated
      tags/ concepts are mentioned in the document

- Once semantic tagging is done, you have extracted in each document
  you are counting <span style="text-decoration:underline">how often a particular
  concept has appeared</span>**.**
- Then you can decide on the overlay. This is the graph-based profile
- First you need the **graph** as an **input,** and **smartness** is
  how you do the **tagging**
    - How? Looks through text and cuts it into words or phrases, (uni,
      bigrams) it maps to the graph. It may not be exact as what is in
      the graph, so we do **approximate tagging**. (similarities,
      synonyms, partial overlays)


### Building concept-based user model
- Nodes represent abstract topics considered interesting to the user,
  rather than specific words or set of words

**First method**
- ![image](https://user-images.githubusercontent.com/33334078/95708030-2d536580-0c96-11eb-85a6-11b055f3b65f.png)

- We take each document and do semantic tagging to get the overlay
- From then on, you need to come up with **aggregated list of
  concepts** – look for top concepts. May **overlay** the graph
  that are sparse or big.
    - Might need to do pre-processing on the graph.
        - E.g. Common categories, most frequent concepts to come
          up with list based the counting on the graph


**Second method**
- ![image](https://user-images.githubusercontent.com/33334078/95708043-393f2780-0c96-11eb-8539-f205b14f5750.png)

- Identified positive documents, then based on that you need to
  identify what are the common things in these documents
  - Can **cluster** the documents the most similar documents, then from
    this group, then **extract topics** for each of the clusters then
    come up with the **top concepts** as ur user model
  - User modelling component is the red bits
  - But the input needs to be reliable \!\! the positive examples



## Summary
- User Information Collection
  - Explicit: given by the user
  - Implicit: monitoring what the user is doing, collected by the
      system
- If we do implicit information collection:
  - Step 1: Identify the user
      - Depends on the data collection
  - Step 2: Construct the model
      - Keyword-based
      - Graph-based
      - Concept-based
      - We ned to think about how the model is represented and what
        the input data is.
